#!/usr/bin/env ruby

autoload :YAML, 'yaml'
autoload :JSON, 'json'
autoload :FileUtils, 'fileutils'
autoload :Find, 'find'
require 'optparse'

def run(cmd)
  unless system(cmd)
    puts "Process exited with code #{$?.exitstatus}: #{cmd}"
    exit($?.exitstatus)
  end
end

def get_output(cmd)
  IO.popen(cmd) do |io|
    io.read
  end
end

# Loads YAML using YAML 1.2 behavior.
# In particular this does not map on/yes to true.
# See https://stackoverflow.com/questions/28507195/yaml-ruby-loading-on-as-true
# and https://stackoverflow.com/questions/36463531/pyyaml-automatically-converting-certain-keys-to-boolean-values
def load_yaml_12(src)
  doc = YAML.parse(src)
  doc.select{ |node| node.is_a?(Psych::Nodes::Scalar) &&
                   %w(on off).include?(node.value) }
    .each{|node| node.quoted = true }
  doc.to_ruby
end

DEBUGGER_SCENARIOS = %w,
  debugger_probes_status

  debugger_method_probes_snapshot
  debugger_line_probes_snapshot
  debugger_mix_log_probe

  debugger_pii_redaction

  debugger_exception_replay
  debugger_expression_language
,

options = {
}
OptionParser.new do |opts|
  opts.banner = "Usage: str [options]"

  opts.on("--rebuild", "Rebuild everything") do
    options[:rebuild] = true
  end

  opts.on("--build-only", "Only build, do not run tests") do
    options[:build_only] = true
  end

  opts.on('-L', "--language=LANG", "Test against tracer in specified language") do |v|
    options[:language] = v
  end

  opts.on("--ruby", "Test against Ruby library's most recent release") do
    options[:language] = 'ruby'
    options.delete(:dev)
  end

  opts.on("--ruby-dev", "Test against local develompent tree of Ruby library") do
    options[:dev] = true
    options[:language] = 'ruby'
  end

  opts.on("--python", "Test against Python tracer") do
    options[:language] = 'python'
  end

  opts.on("--java", "Test against Java tracer") do
    options[:language] = 'java'
  end

  opts.on('-V', '--variant=VARIANT', 'Specify weblog variant') do |v|
    options[:weblog_variant] = v
  end

  opts.on('-d', "--debugger", "Run all dynamic instrumentation / \"debugger\" tests") do
    options[:debugger] = true
  end

  opts.on("--debugger-ruby", "Run debugger scenarios implemented in Ruby") do
    options[:scenarios] ||= []
    options[:scenarios] += %w,
      DEBUGGER_PII_REDACTION
      DEBUGGER_PROBES_SNAPSHOT
      DEBUGGER_PROBES_STATUS
    ,
    options.delete(:debugger)
  end

  opts.on("--debugger-pii", "Run debugger_pii_redaction scenario") do
    options[:scenarios] ||= []
    options[:scenarios] << 'debugger_pii_redaction'
    options.delete(:debugger)
  end

  opts.on("-s", "--scenario SCENARIO", "Specify scenario to run (in any case, can be used more than once)") do |v|
    options[:scenarios] ||= []
    options[:scenarios] << v
  end

  opts.on("-t", "--test-file TEST", "Specify test file to run (can be used more than once)") do |v|
    options[:tests] ||= []
    options[:tests] << v
  end

  opts.on('-m', '--test-method METHOD', 'Specify test method or pattern to execute') do |v|
    options[:test_method] = v
  end

  opts.on('-v', "--verbose", "Request verbose pytest output") do
    options[:verbose] = true
  end

  opts.on("--verify", "Do not run the tests but verify results using the current system tests' test code (\"replay\" mode). Only works when one test is executed") do
    options[:verify_only] = true
  end

  opts.on('--generate-forced-tests-list', 'Generate forced tests list') do
    options[:generate_forced_tests_list] = true
  end

  opts.on('--crop-system-tests-to-debugger', 'Crop system tests in tracer to debugger only') do
    doc = load_yaml_12(File.read('.github/workflows/system-tests.yml'))
    variant = (options[:weblog_variant] ||= 'rails80')
    %w(build-apps test aggregate).each do |job|
      doc['jobs'][job]['strategy']['matrix']['app'] = [variant]
    end
    doc['jobs']['test']['strategy']['matrix']['scenario'] = %w,DEFAULT,
    doc['jobs']['test']['strategy']['matrix']['include'].delete_if do |spec|
      spec['scenario'] !~ /DEBUGGER/
    end
    %w(cleanup).each do |job|
      images = doc['jobs'][job]['strategy']['matrix']['image'].reject do |image|
        image =~ /\Aweblog-/ && !image.include?(variant)
      end
      doc['jobs'][job]['strategy']['matrix']['image'] = images
    end
    File.open('.github/workflows/system-tests.yml', 'w') do |f|
      f << YAML.dump(doc)
    end
    exit
  end

  opts.on('--show-tests', 'Show defined tests') do
    options[:show_tests] = true
  end

  opts.on('--builder-help', 'Show system tests builder help') do
    options[:builder_help] = true
  end

  opts.on('--runner-help', 'Show system tests runner help') do
    options[:runner_help] = true
  end

  opts.on('--no-build', 'Do not build images - use previously built onse') do
    options[:build] = false
  end
end.parse!

unless options[:tests]
  # Run debugger tests by default, since default test set is not useful.
  warn 'scoping testst to debugger since none were requested'
  options[:debugger] = true
end

# Verify current path is in system-tests
unless File.exist?('run.sh') && File.exist?('manifests')
  raise "Expected to be in system-tests"
end

if options[:builder_help]
  run("./build.sh -h")
  exit
end

if options[:runner_help]
  run("./run.sh +h")
  exit
end

if options[:generate_forced_tests_list]
  scenarios = DEBUGGER_SCENARIOS
  scenario_files = {}
  scenarios.each do |scenario|
    Find.find('tests/debugger') do |path|
      if File.basename(path) =~ /\Atest_debugger_.*\.py\z/
        contents = File.read(path)
        contents.scan(/@scenarios.#{scenario}([^:]+)/) do |match|
          if $1 =~ /\bclass (\w+)/
            scenario_files[scenario.upcase] ||= []
            scenario_files[scenario.upcase] << "#{path}::#{$1}"
          end
        end
      end
    end
  end
  puts JSON.pretty_generate(scenario_files)
  exit
end

language = options[:language] || 'ruby'
if language == 'node'
  language = 'nodejs'
end

if language == 'ruby'
  if options[:dev]
    run('rsync -av ~/apps/dtr/ binaries/dd-trace-rb --exclude .git --delete')
  elsif
    # A bit sketchy...
    run('rm -rf binaries/dd-trace-rb')
  end
end

def decide_python_version
  dirs = ENV.fetch('PATH').split(':')
  %w(3.13 3.12).each do |version|
    dirs.each do |dir|
      if File.exist?(File.join(dir, "python#{version}"))
        return version
      end
    end
  end
  raise "No compatible python versions found on system"
end

# If we are only verifying, we do not need to build.
# We also do not really need to patch since there isn't too much work
# happening during the running phase.
if options[:build] != false && !options[:verify_only]
  if options[:rebuild] || !File.exist?('venv')
    # I'm sure venv is created by system tests somehow...
    # This didn't happen after I deleted it due to system tests now
    # using Python 3.11+ features.
    # Plus from what I see in build.sh, system tests tries to use either
    # python 3.12 or 3.9 and currently ubuntu offers 3.10 and 3.11.
    # So, create venv manually here.
    python_version = decide_python_version
    FileUtils.rm_rf('venv')
    run("virtualenv -p python#{python_version} venv")
    run('. venv/bin/activate && venv/bin/pip install -r requirements.txt --break-system-packages')
  end

  variant = options[:weblog_variant] || if options[:ruby]
    'rails80'
  end
  variant_option = if variant
    "--weblog-variant #{variant}"
  end
  cmd = "./utils/build/build.sh #{variant_option} #{language}"
  if options[:verbose]
    cmd = "bash -x #{cmd}"
    puts "Run: #{cmd}"
  end
  run(cmd)

  # Patch pytest to report collected tests before running in verbose mode.
  # There is no upstream option to do so; there is apparently an option to
  # make it report the collected tests but not run anything, and we could
  # possibly use this but at an increase in runtime and then we would still
  # need to patch system tests shell scripts to request that command to be
  # executed.
  Find.find('venv') do |path|
    if File.basename(path) == 'terminal.py' && path =~ %r,_pytest/terminal.py\z,
      patch_path = File.realpath(File.join(File.dirname(File.realpath(__FILE__)), '../patches/pytest/terminal.py.patch'))
      unless File.read(path).include?('system-tests-runner:')
        puts "Patching pytest"
        Dir.chdir(File.dirname(path)) do
          run("patch -p1 < #{patch_path}")
        end
      end
    end
  end

  # If we are not verifying, delete existing logs.
  Dir.entries('.').select do |entry|
    entry =~ /\Alogs(_|\z)/
  end.each do |entry|
    FileUtils.rm_rf(entry)
  end
end

exit if options[:build_only]

selected_scenarios = options[:scenarios] || if options[:debugger]
  DEBUGGER_SCENARIOS
else
  []
end
selected_scenarios = Set.new(selected_scenarios)

class TestFile
  def initialize(path:)
    @path = path
  end

  attr_reader :path

  def rel_path
    path[6..]
  end

  def comps
    @comps ||= begin
      basename = File.basename(path)
      comps = File.dirname(path).split('/').map { |part| part + '/' } + [basename]
    end
  end

  def python_mod_name
    mod_name = rel_path.sub(%r,\.py\z,, '').split('/').join('.')
  end

  attr_accessor :test_classes
end

class TestClass
  def initialize(name:, scenario:, test_methods:)
    @name = name
    @scenario = scenario
    @test_methods = test_methods
  end

  attr_reader :name
  attr_reader :scenario
  attr_reader :test_methods
end

def all_tests
  @all_tests ||= [].tap do |tests|
    Find.find('tests') do |path|
      if %w(test_the_test __pycache__).include?(File.basename(path))
        Find.prune
      end

      next unless File.file?(path) && File.basename(path).start_with?('test_')

      test = TestFile.new(path: path)

      tests << test
    end
  end
end

# --show-tests outputs test file basenames, but here we need paths.
# Try to convert - if there is only one path matching the basename,
# replace the basename with a path. If a test does not exist, fail here
# rather than after system tests does a bunch of useless work.
options[:tests]&.map! do |test|
  unless File.exist?(test)
    if File.basename(test) == test
      eligible = all_tests.select do |candidate|
        File.basename(candidate) == test
      end
      if eligible.length == 1
        warn "Using #{eligible.first} instead of #{test}"
        test = eligible.first
      end
    end

    unless File.exist?(test)
      raise "Test does not exist: #{test}"
    end
  end
  test
end

if options[:show_tests]
  run('venv/bin/pip install -r requirements.txt')
  manifest_data = YAML.load(File.read("manifests/#{language}.yml"))
  if options[:debugger]
    manifest_data['tests/'].delete_if do |k, v|
      k != 'debugger/'
    end
  end
  test_info_path = File.join(File.dirname(File.realpath(__FILE__)), '../support/test-info.py')
  mod_names = all_tests.map do |test|
    if selected_tests = options[:tests]
      unless selected_tests.include?(test.path)
        next
      end
    end

    test_map = manifest_data.dig(*test.comps)
    unless test_map
      # I think these are used to test ST itself
      warn "Test not in manifest? #{test.comps.join}"
      next
    end
    if String === test_map && test_map =~ /\Amissing_feature\b/
      # Not applicable to the current language, skip
      next
    end
    unless Hash === test_map
      # TODO test_map can be "missing_feature", this should be handled
      raise "Test map is not a hash: #{test_map}"
    end

    test.python_mod_name
  end

  cmd = "env PYTHONPATH=tests venv/bin/python #{test_info_path} #{mod_names.join(' ')}"
  puts(cmd)
  info = get_output(cmd)
  if $?.exitstatus != 0
    raise "Failed to run: #{cmd}"
  end
  info = JSON.parse(info)

  all_tests.each do |test|
    this_info = info[test.python_mod_name]
    next unless this_info

    test.test_classes = this_info.map do |cls_name, meta|
      TestClass.new(name: cls_name, scenario: meta.fetch('scenario'), test_methods: meta['methods'])
    end

    test.test_classes.each do |cls|
      puts "#{test.path}::#{cls.name} scenario: #{cls.scenario}"
      cls.test_methods.each do |meth|
        puts "  #{meth}"
      end
    end
  end
  exit
end

run_args = ''

parametric = options[:tests]&.any? do |test|
  test =~ %r,\Atests/parametric/,
end
=begin not needed?
if parametric
  run_args << ' PARAMETRIC'
end
=end

if parametric
  run_args << " -L #{language}"
else
  run_args << " --library #{language}"
end

scenarios_added = false
if tests = options[:tests]
  add_scenarios = options[:scenarios].nil?
  tests.each do |test|
    if add_scenarios
      if File.exist?(test)
        File.open(test) do |f|
          f.each_line do |line|
            if line =~ /\A@scenarios.(\w+)/
              options[:scenarios] ||= []
              unless selected_scenarios.include?($1)
                selected_scenarios << $1
                puts "adding scenario #{$1} for test file #{test}"
                scenarios_added = true
              end
            end
          end
        end
      else
        warn "file does not exist: #{test}"
      end
    end
    run_args << " #{test}"
  end
end

# TODO this isn't working.
# ST runs each scenario separately.
# pytest fails if a scenario has no enabled tests.
# This means we cannot simply add default scenario, we need to only
# do so if there is at least one test that doesn't have a scenario
# specified.
#
# Note also that when a scenario skips everything we don't get the
# output of all tests that exist, making troubleshooting even more
# obnoxious.
#
# And ST breaks pytest's feature to show collected tests.
=begin
if scenarios_added
  selected_scenarios << 'default'
end
if selected_scenarios.include?('default')
  selected_scenarios.delete('default')
  selected_scenarios = %w,default, + selected_scenarios.to_a
end
=end

selected_scenarios.each do |scenario|
  run_args << " +S #{scenario}"
end

if options[:verbose]
  run_args << " -v -s"
  ENV['STR_VERBOSE'] = '1'
end

if options[:verify_only]
  run_args << " --replay"
end

if method = options[:test_method]
  run_args << " -k #{method}"
end

cmd = "./run.sh #{run_args}"
if options[:verbose]
  cmd = "bash -x #{cmd}"
end

puts "Executing: #{cmd}"
run(cmd)
